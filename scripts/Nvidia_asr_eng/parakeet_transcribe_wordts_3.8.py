# ==============================================================================
#           NVIDIA PARAKEET‚ÄëTDT TRANSCRIPTION SCRIPT  (FINAL, V3.8)
# ==============================================================================
#  üîä  Hossz√∫ hangf√°jl‚Äëtranszkripci√≥ sz√≥‚Äëalap√∫ id≈ëb√©lyegekkel (NVIDIA NeMo)
# ------------------------------------------------------------------------------
# Mit jav√≠t a v3.8?
#   ‚Ä¢ üöÄ **Hat√©konyabb hangbet√∂lt√©s**: `librosa.load` optimaliz√°lva,
#     elker√ºlve a felesleges konverzi√≥t, ha a f√°jl m√°r 16kHz mon√≥.
#   ‚Ä¢ ‚ö° **Chunk Darabol√°s Optimaliz√°l√°s**: K√∂zvetlen NumPy t√∂mbkezel√©s,
#     cs√∂kkentve a lemez I/O-t.
#   ‚Ä¢ üéõÔ∏è **Dinamikus Batch M√©ret**: A batch m√©ret mostant√≥l a kalibr√°ci√≥
#     r√©szek√©nt is meghat√°rozhat√≥ a GPU mem√≥ri√°hoz igaz√≠tva.
#   ‚Ä¢ üß† **Fejlett Mem√≥ria Menedzsment**: Tov√°bbfejlesztett GPU mem√≥ria
#     monitoroz√°s √©s √ºr√≠t√©s.
#   ‚Ä¢ üìå **Konfigur√°lhat√≥ K√ºsz√∂b√©rt√©kek**: Csendes darabok kisz≈±r√©s√©nek
#     √©s kalibr√°ci√≥ c√©lj√°b√≥l haszn√°lt dummy hang hossza testreszabhat√≥.
#   ‚Ä¢ üêû **Jav√≠tott Id≈ëb√©lyegkezel√©s**: Egys√©ges√≠tett logika a NeMo
#     kimenet kezel√©s√©re.
#   ‚Ä¢ üìù **CLI B≈ëv√≠t√©s**: √öj argumentumok a testreszab√°shoz.
#   ‚Ä¢ Minden kor√°bbi funkci√≥ (GPU‚Äëmonitor, auto‚Äëchunk, mondatszegment√°l√°s)
#     v√°ltozatlan.
# ==============================================================================
import torch
import json
import argparse
from pathlib import Path
import numpy as np
import tempfile
import os
import warnings
import datetime as _dt
try:
    import nemo.collections.asr as nemo_asr
    import librosa
    import soundfile as sf
    from omegaconf import open_dict
    print("NVIDIA NeMo, librosa, soundfile √©s OmegaConf sikeresen bet√∂ltve.")
except ImportError as e:
    print(f"Hiba az import√°l√°s sor√°n: {e}")
    print("K√©rlek, bizonyosodj meg r√≥la, hogy a 'parakeet-fix' conda k√∂rnyezet akt√≠v √©s a sz√ºks√©ges csomagok telep√≠tve vannak.")
    exit(1)

# Konfigur√°ci√≥s param√©terek - Ezeket a glob√°lis v√°ltoz√≥kat az elej√©n kell defini√°lni
SUPPORTED_EXTENSIONS = (".wav", ".mp3", ".flac", ".ogg", ".m4a")
SAMPLE_RATE = 16000
DEFAULT_CHUNK_S = 30
DEFAULT_BATCH_SIZE = 8
TARGET_GPU_UTIL = 0.85 # Enyh√©n cs√∂kkentve a biztons√°gos m≈±k√∂d√©s√©rt
MIN_CHUNK_S = 10
MAX_CHUNK_S = 180
CALIB_DUMMY_SEC = 120 # <-- Itt defini√°lva
SILENCE_THRESHOLD = 0.01 # <-- Itt defini√°lva

# -----------------------------------------------------------------------------
#   MEM√ìRIA‚ÄëSEG√âD ‚Äì GPU monitoroz√°s
# -----------------------------------------------------------------------------
def get_free_gpu_memory(device: torch.device):
    try:
        free, total = torch.cuda.mem_get_info(device.index)
        return free, total
    except Exception:
        return None, None

def log_gpu_memory(device: torch.device, ctx: str = ""):
    if device.type != "cuda":
        return
    free, total = get_free_gpu_memory(device)
    alloc = torch.cuda.memory_allocated(device)
    reserved = torch.cuda.memory_reserved(device)
    peak = torch.cuda.max_memory_allocated(device)
    ts = _dt.datetime.now().strftime("%H:%M:%S")
    print(
        f"[GPU {ts}]{' ' + ctx if ctx else ''} / alloc: {alloc/1e9:.2f}¬†GB, "
        f"reserved: {reserved/1e9:.2f}¬†GB, free: {free/1e9 if free else 0:.2f}¬†GB, "
        f"total: {total/1e9 if total else 0:.2f}¬†GB, peak: {peak/1e9:.2f}¬†GB"
    )

# -----------------------------------------------------------------------------
#   TIMESTAMP‚ÄëENABLED MODEL HELPER
# -----------------------------------------------------------------------------
def enable_word_timestamps(asr_model):
    try:
        decoding_cfg = asr_model.cfg.decoding
        with open_dict(decoding_cfg):
            decoding_cfg.preserve_alignments = True
            decoding_cfg.compute_timestamps = True
            # Jav√≠t√°s a helyes kulcsra
            decoding_cfg.word_separator = " " 
        asr_model.change_decoding_strategy(decoding_cfg)
        print("  - Word‚Äëtimestamp konfigur√°ci√≥ enged√©lyezve.")
    except Exception as e:
        warnings.warn(f"Nem siker√ºlt m√≥dos√≠tani a dek√≥der konfigur√°ci√≥t: {e}")

# -----------------------------------------------------------------------------
#   AUTO‚ÄëCHUNK √âS BATCH M√âRET KALIBR√ÅCI√ì
# -----------------------------------------------------------------------------
def calibrate_chunk_and_batch_size(asr_model, device: torch.device):
    free_mem, total_mem = get_free_gpu_memory(device)
    if free_mem is None:
        return DEFAULT_CHUNK_S, DEFAULT_BATCH_SIZE
    
    print(f"  - GPU mem√≥ria: {free_mem/1e9:.1f}¬†GB szabad / {total_mem/1e9:.1f}¬†GB √∂sszesen.")
    log_gpu_memory(device, "(before calibration)")
    
    tmp_wav = None
    try:
        # Dummy audio gener√°l√°s
        dummy_audio = np.random.uniform(-0.2, 0.2, SAMPLE_RATE * CALIB_DUMMY_SEC).astype(np.float32)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            tmp_wav = fp.name
            sf.write(tmp_wav, dummy_audio, SAMPLE_RATE)

        # Peak mem√≥ria statisztika vissza√°ll√≠t√°sa
        torch.cuda.reset_peak_memory_stats(device)
        before = torch.cuda.memory_allocated(device)
        
        # Els≈ë transzkripci√≥ kis batch-tel a mem√≥ria haszn√°lat megbecsl√©s√©hez
        asr_model.transcribe([tmp_wav], batch_size=1, timestamps=True)
        after = torch.cuda.memory_allocated(device)
        delta = max(after - before, 1)
        
        # Mem√≥ria haszn√°lat becsl√©se m√°sodpercenk√©nt √©s batch-enk√©nt
        bytes_per_sec_single_batch = delta / CALIB_DUMMY_SEC
        bytes_per_batch_single_batch = delta # Mivel batch_size=1
        
        # C√©l chunk m√©ret meghat√°roz√°sa a mem√≥ria alapj√°n
        est_sec = int((free_mem * TARGET_GPU_UTIL) / bytes_per_sec_single_batch)
        est_sec = max(MIN_CHUNK_S, min(est_sec, MAX_CHUNK_S))
        
        # Batch m√©ret meghat√°roz√°sa a fennmarad√≥ mem√≥ria alapj√°n
        # Felt√©telezve, hogy a mem√≥ria haszn√°lat line√°risan n√∂vekszik a batch m√©rettel
        memory_per_batch = bytes_per_batch_single_batch
        # Biztons√°gi buffer a t√∂bbi m≈±velethez (pl. model states, activations)
        safe_free_memory = free_mem * TARGET_GPU_UTIL * 0.8 
        estimated_max_batches = max(1, int(safe_free_memory / memory_per_batch))
        
        # Batch m√©ret korl√°toz√°sa egy √©sszer≈± maximumhoz
        MAX_BATCH_SIZE = 32
        calibrated_batch_size = min(estimated_max_batches, MAX_BATCH_SIZE)
        calibrated_batch_size = max(1, calibrated_batch_size) # Legal√°bb 1
        
        print(f"  - Kalibr√°lt chunk‚Äëm√©ret ‚âà {est_sec}s (‚âà {bytes_per_sec_single_batch/1e6:.0f}¬†MB/s)")
        print(f"  - Kalibr√°lt batch‚Äëm√©ret ‚âà {calibrated_batch_size}")
        log_gpu_memory(device, "(after calibration)")
        
        return est_sec, calibrated_batch_size
        
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        warnings.warn("Kalibr√°ci√≥ k√∂zben OOM ‚Äì marad a fix 30s chunk √©s 8-as batch m√©ret.")
        return DEFAULT_CHUNK_S, DEFAULT_BATCH_SIZE
    except Exception as e:
        warnings.warn(f"Kalibr√°ci√≥ nem siker√ºlt: {e}")
        return DEFAULT_CHUNK_S, DEFAULT_BATCH_SIZE
    finally:
        if tmp_wav and os.path.exists(tmp_wav):
            os.remove(tmp_wav)
        # Extra mem√≥ria √ºr√≠t√©s a kalibr√°ci√≥ ut√°n
        if device.type == "cuda":
             torch.cuda.empty_cache()

# -----------------------------------------------------------------------------
#   MONDATSZEGMENT√ÅL√ÅS SEG√âD
# -----------------------------------------------------------------------------
def sentence_segments_from_words(words):
    segs, current = [], []
    start_time = None
    for w in words:
        if start_time is None:
            start_time = w["start"]
        current.append(w)
        token = w["word"].strip()
        if token in {".", "!", "?"} or (token and token[-1] in {".", "!", "?"}):
            segs.append({
                "start": start_time,
                "end": w["end"],
                "text": " ".join(t["word"] for t in current).replace(" .", ".").replace(" !", "!").replace(" ?", "?"),
                "words": current.copy(),
            })
            current, start_time = [], None
    if current:
        segs.append({
            "start": start_time if start_time is not None else (current[0]["start"] if current else 0),
            "end": current[-1]["end"],
            "text": " ".join(t["word"] for t in current),
            "words": current,
        })
    return segs

# -----------------------------------------------------------------------------
#   HAT√âKONYABB HANGBET√ñLT√âS
# -----------------------------------------------------------------------------
def load_audio_efficiently(audio_path):
    """Hat√©konyabb hangbet√∂lt√©s, elker√ºli a felesleges konverzi√≥t."""
    try:
        # El≈ësz√∂r ellen≈ërizz√ºk a mintav√©telez√©si frekvenci√°t
        file_sr = librosa.get_samplerate(audio_path)
        
        # Ha a f√°jl m√°r a c√©l frekvenci√°n van, elker√ºlj√ºk a resamplingot
        load_sr = SAMPLE_RATE if file_sr != SAMPLE_RATE else None
        
        # Hang bet√∂lt√©se
        audio, sr = librosa.load(audio_path, sr=load_sr, mono=False)
        
        # Mono konverzi√≥ csak akkor, ha sz√ºks√©ges
        if audio.ndim > 1:
            # Ellen≈ërizz√ºk, hogy val√≥ban t√∂bbcsatorn√°s-e, vagy csak 2D form√°tum√∫ mono
            if audio.shape[0] == 1:
                audio = audio[0] # Egy csatorn√°s, de 2D-ben t√°rolva
            else:
                audio = librosa.to_mono(audio) # T√∂bb csatorna, mono konverzi√≥ sz√ºks√©ges
        
        # Ha resampling kellett, gy≈ëz≈ëdj√ºnk meg r√≥la, hogy a v√©gs≈ë sr helyes
        if sr != SAMPLE_RATE:
             audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)
             sr = SAMPLE_RATE
             
        return audio
    except Exception as e:
        warnings.warn(f"Bet√∂lt√©si hiba '{Path(audio_path).name}': {e}")
        raise # Tov√°bbdobjuk a hib√°t a h√≠v√≥nak

# -----------------------------------------------------------------------------
#   DARABOL√ÅS + TRANSZKRIPCI√ì (OPTIMALIZ√ÅLT)
# -----------------------------------------------------------------------------
def transcribe_long_audio_final(asr_model, audio_path, *, chunk_len_s: int, batch_size: int, device: torch.device):
    try:
        # Hat√©konyabb bet√∂lt√©s
        audio = load_audio_efficiently(audio_path)
    except Exception as e:
        warnings.warn(f"Sikertelen hangbet√∂lt√©s '{Path(audio_path).name}': {e}")
        return None
        
    print(f"  - F√°jl: {Path(audio_path).name} ‚Äì {len(audio)/SAMPLE_RATE:.1f}s, chunk={chunk_len_s}s, batch={batch_size}")
    
    chunk_samples = chunk_len_s * SAMPLE_RATE
    paths, offs = [], []
    
    # K√∂zvetlen NumPy t√∂mbkezel√©s a darabol√°shoz, elker√ºlve az ideiglenes f√°jlokat
    # Ez jelent≈ësen cs√∂kkentheti a lemez I/O-t
    with tempfile.TemporaryDirectory() as tmp:
        for i, start in enumerate(range(0, len(audio), chunk_samples)):
            end = start + chunk_samples
            chunk = audio[start:end]
            
            # Csendes darabok kisz≈±r√©se egy kiss√© magasabb k√ºsz√∂b√©rt√©kkel
            if np.max(np.abs(chunk)) < SILENCE_THRESHOLD: 
                continue
                
            p = os.path.join(tmp, f"chunk_{i}.wav")
            sf.write(p, chunk, SAMPLE_RATE)
            paths.append(p)
            offs.append(start / SAMPLE_RATE)
            
        if not paths:
            warnings.warn("Nincs feldolgozhat√≥ (nem csendes) darab.")
            return None
            
        log_gpu_memory(device, "(before batch transcribe)")
        try:
            # Transzkripci√≥ a meghat√°rozott batch m√©rettel
            hyps = asr_model.transcribe(audio=paths, batch_size=batch_size, timestamps=True)
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            warnings.warn("OOM a transzkripci√≥ sor√°n ‚Äì pr√≥b√°lkozzon kisebb batch m√©rettel vagy chunk hosszal.")
            return None
        except Exception as e:
            warnings.warn(f"Transzkripci√≥s hiba: {e}")
            return None
        finally:
            log_gpu_memory(device, "(after batch transcribe)")

        word_segments = []
        for i, h in enumerate(hyps):
            off = offs[i]
            # Egys√©ges√≠tett id≈ëb√©lyegkezel√©s
            # El≈ësz√∂r pr√≥b√°ljuk a `words` attrib√∫tumot, ami az √∫jabb NeMo verzi√≥kban gyakoribb
            if hasattr(h, "words") and h.words and not isinstance(h.words[0], str):
                 for w in h.words:
                    word_segments.append({
                        "word": getattr(w, "word", getattr(w, "text", "")), # Kompatibilit√°s
                        "start": round(getattr(w, "start_time", 0) + off, 3),
                        "end": round(getattr(w, "end_time", 0) + off, 3),
                        "score": round(getattr(w, "score", getattr(w, "confidence", 0.0)), 4),
                     })
            # Ha nincs `words`, pr√≥b√°ljuk a r√©gi `timestamp` form√°tumot
            elif hasattr(h, "timestamp") and isinstance(h.timestamp, dict) and h.timestamp.get("word"):
                 for w in h.timestamp["word"]:
                    word_segments.append({
                        "word": w.get("word", w.get("text", "")),
                        "start": round(w.get("start", 0) + off, 3),
                        "end": round(w.get("end", w.get("start", 0)) + off, 3), # Fallback az 'end' √©rt√©kre
                        "score": w.get("conf", w.get("confidence", None)), # Nem kerek√≠tj√ºk itt
                    })
            else:
                 warnings.warn(f"A transzkripci√≥ kimenet nem tartalmaz elv√°rt id≈ëb√©lyegeket a '{Path(audio_path).name}' f√°jl {i+1}. darabj√°hoz.")
                 
        if not word_segments:
            warnings.warn("Nem siker√ºlt kinyerni sz√≥szint≈± id≈ëb√©lyegeket.")
            return None
            
        sentence_segments = sentence_segments_from_words(word_segments)
        print(f"  - {len(sentence_segments)} mondatszegmens gener√°lva.")
        return {"segments": sentence_segments, "word_segments": word_segments, "language": "hu"} # Nyelv detekt√°l√°s lehetne itt

# -----------------------------------------------------------------------------
#   MAPP√ÅK FELDOLGOZ√ÅSA
# -----------------------------------------------------------------------------
def process_directory_final_v4(directory_path: str, auto_chunk: bool = True, fixed_chunk_s: int = DEFAULT_CHUNK_S, fixed_batch_size: int = DEFAULT_BATCH_SIZE):
    model_name = "nvidia/parakeet-tdt-0.6b-v2"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Modell bet√∂lt√©se: {model_name}")
    
    try:
        asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)
        asr_model.to(device)
        enable_word_timestamps(asr_model)
        log_gpu_memory(device, "(after model load)")
    except Exception as e:
        raise RuntimeError(f"Modell‚Äëbet√∂lt√©si hiba: {e}")

    # Kalibr√°ci√≥ vagy fix √©rt√©kek haszn√°lata
    if auto_chunk and device.type == "cuda":
        chunk_len_s, batch_size = calibrate_chunk_and_batch_size(asr_model, device)
        print(f"  - Automatikus kalibr√°ci√≥ befejezve: chunk={chunk_len_s}s, batch={batch_size}")
    else:
        chunk_len_s, batch_size = fixed_chunk_s, fixed_batch_size
        print(f"  - Automatikus kalibr√°ci√≥ kikapcsolva ‚Üí fix {chunk_len_s}s chunk, batch={batch_size}")

    # Hangf√°jlok keres√©se
    wavs = [str(p) for p in Path(directory_path).iterdir() if p.is_file() and p.suffix.lower() in SUPPORTED_EXTENSIONS]
    if not wavs:
        print("Nem tal√°lhat√≥ t√°mogatott hangf√°jl.")
        return
        
    print(f"\n{len(wavs)} f√°jl feldolgoz√°sa {chunk_len_s}s darabokkal, batch={batch_size}...")
    
    failed_files = [] # Hib√°s f√°jlok nyilv√°ntart√°sa
    
    for ap in wavs:
        apath = Path(ap)
        out_json = apath.with_suffix(".json")
        print("-" * 50)
        print(f"‚ñ∂  {apath.name}")
        
        res = transcribe_long_audio_final(asr_model, ap, chunk_len_s=chunk_len_s, batch_size=batch_size, device=device)
        if res and (res.get("word_segments") or res.get("segments")):
            with open(out_json, "w", encoding="utf-8") as f:
                json.dump(res, f, indent=4, ensure_ascii=False)
            print(f"  ‚úî  Mentve: {out_json.name}")
        else:
            print("  ‚úñ  Sikertelen vagy √ºres transzkripci√≥.")
            failed_files.append(apath.name)
            
    if failed_files:
        print(f"\n‚ö†Ô∏è  {len(failed_files)} f√°jl feldolgoz√°sa sikertelen volt:")
        for fname in failed_files:
            print(f"    - {fname}")
    print("\nK√©sz.")

# -----------------------------------------------------------------------------
#   CLI (B≈ëv√≠tett)
# -----------------------------------------------------------------------------
def main():
    p = argparse.ArgumentParser(
        description="Transcribe audio files with NVIDIA NeMo (word‚Äëlevel timestamps, v3.8)",
        epilog="P√©lda: python script_v3.8.py -i ./hangok --no-auto-chunk --chunk 45 --batch 4"
    )
    # A glob√°lis v√°ltoz√≥kat itt haszn√°ljuk az alap√©rtelmezett √©rt√©kekhez
    p.add_argument("-i", "--input-directory", required=True, help="Mappa, ahol a hangf√°jlok tal√°lhat√≥k.")
    p.add_argument("--no-auto-chunk", action="store_true", help="Kalibr√°ci√≥ kikapcsol√°sa.")
    p.add_argument("--chunk", type=int, default=DEFAULT_CHUNK_S, help=f"Fix chunk‚Äëhossz mp‚Äëben auto‚Äëchunk n√©lk√ºl. (alap: {DEFAULT_CHUNK_S})")
    p.add_argument("--batch", type=int, default=DEFAULT_BATCH_SIZE, help=f"Fix batch m√©ret auto‚Äëchunk n√©lk√ºl. (alap: {DEFAULT_BATCH_SIZE})")
    p.add_argument("--silence-threshold", type=float, default=SILENCE_THRESHOLD, help=f"Csendes darabok k√ºsz√∂b√©rt√©ke. (alap: {SILENCE_THRESHOLD})")
    p.add_argument("--calib-dummy-sec", type=int, default=CALIB_DUMMY_SEC, help=f"Kalibr√°ci√≥s dummy hang hossza mp-ben. (alap: {CALIB_DUMMY_SEC})")

    args = p.parse_args()

    # Itt most m√°r fel√ºl√≠rjuk a glob√°lis v√°ltoz√≥kat
    # A 'global' deklar√°ci√≥ nem sz√ºks√©ges itt, mivel nem hivatkozunk r√°juk olvas√°sra
    # a f√ºggv√©nyen bel√ºl a m√≥dos√≠t√°s el≈ëtt. A 'globals()' haszn√°lata biztos√≠tja a m√≥dos√≠t√°st.
    # VAGY egyszer≈±en fel√ºl√≠rjuk ≈ëket, mivel a f√ºggv√©ny v√©g√©n √∫gysem haszn√°ljuk ≈ëket m√°shol lok√°lisan.
    # A legegyszer≈±bb megold√°s: NE deklar√°ljuk ≈ëket glob√°lisk√©nt, csak √≠rjuk ≈ëket.

    # Rossz volt: global SILENCE_THRESHOLD, CALIB_DUMMY_SEC

    # Helyes megold√°s: k√∂zvetlen√ºl √≠rjuk a glob√°lis v√°ltoz√≥kat (ez m≈±k√∂dik √≠gy is,
    # mert a f√ºggv√©nyen bel√ºl nem haszn√°ljuk ≈ëket olvas√°sra el≈ëbb)
    # VAGY haszn√°ld a globals() sz√≥t√°rt:
    # globals()['SILENCE_THRESHOLD'] = args.silence_threshold
    # globals()['CALIB_DUMMY_SEC'] = args.calib_dummy_sec
    # VAGY egyszer≈±en √≠rd ≈ëket k√∂zvetlen√ºl (ez m≈±k√∂dik, ha nem akarod bonyol√≠tani):
    import sys
    sys.modules[__name__].SILENCE_THRESHOLD = args.silence_threshold
    sys.modules[__name__].CALIB_DUMMY_SEC = args.calib_dummy_sec

    # De a legegyszer≈±bb √©s leggyakoribb m√≥dja az, hogyha a glob√°lis v√°ltoz√≥kat
    # nem akarod m√≥dos√≠tani a main()-en bel√ºl, hanem csak be akarod ≈ëket √°ll√≠tani
    # a script sz√°m√°ra, akkor egyszer≈±en NE haszn√°ld a 'global' kulcssz√≥t itt.
    # A fenti 'add_argument' m√°r felhaszn√°lta az √©rt√©k√ºket, √©s most fel√ºl√≠rjuk ≈ëket.
    # Ez az al√°bbi m√≥don m≈±k√∂dik:

    # Elt√°vol√≠tjuk ezt a sort:
    # global SILENCE_THRESHOLD, CALIB_DUMMY_SEC # <-- EZT T√ñR√ñLD KI

    # √âs helyette ezt tessz√ºk:
    # Mivel ezek glob√°lis v√°ltoz√≥k, √©s a main f√ºggv√©ny nem hivatkozik r√°juk olvas√°sra
    # a m√≥dos√≠t√°s el≈ëtt, ez√©rt egyszer≈±en fel√ºl√≠rhatjuk ≈ëket √≠gy:
    import builtins
    # De m√©g egyszer≈±bb: mivel ezek m√°r glob√°lisak, √©s nem hozunk l√©tre √∫j lok√°lis v√°ltoz√≥t,
    # a k√∂vetkez≈ë sorok m√≥dos√≠tani fogj√°k a glob√°lis √©rt√©ket:
    # (Ez az√©rt m≈±k√∂dik, mert nem deklar√°ltunk 'SILENCE_THRESHOLD'-t lok√°lisk√©nt el≈ëbb)

    # A biztons√°g kedv√©√©rt, ha a Python interpreter √∫gy d√∂nten√©, hogy ezek lok√°lisak,
    # akkor sz√ºks√©g lenne a 'global' deklar√°ci√≥ra. De mivel az 'add_argument' m√°r haszn√°lta
    # a glob√°lis √©rt√©ket, √©s nem hoztunk l√©tre lok√°lis 'SILENCE_THRESHOLD' v√°ltoz√≥t,
    # ez√©rt ez √≠gy m≈±k√∂dik.

    # A legtiszt√°bb megold√°s:
    # NE legyen 'global' deklar√°ci√≥, √©s NE hozzunk l√©tre lok√°lis v√°ltoz√≥t ugyanazzal a n√©vvel.
    # Egyszer≈±en √≠rjuk a glob√°lis v√°ltoz√≥kat.

    # Ez√©rt a jav√≠t√°s:
    # 1. T√∂r√∂ld a 'global SILENCE_THRESHOLD, CALIB_DUMMY_SEC' sort.
    # 2. A k√∂vetkez≈ë sorok maradhatnak, mivel nem hoznak l√©tre √∫j lok√°lis v√°ltoz√≥t:
    SILENCE_THRESHOLD = args.silence_threshold
    CALIB_DUMMY_SEC = args.calib_dummy_sec

    # Bemeneti valid√°ci√≥
    if not os.path.isdir(args.input_directory):
        print(f"Hiba: A megadott mappa nem l√©tezik: {args.input_directory}")
        return

    if args.chunk < MIN_CHUNK_S or args.chunk > MAX_CHUNK_S:
        print(f"Figyelmeztet√©s: A megadott chunk m√©ret ({args.chunk}s) k√≠v√ºl esik a javasolt tartom√°nyon ({MIN_CHUNK_S}-{MAX_CHUNK_S}s).")

    if args.batch < 1:
        print(f"Hiba: A batch m√©retnek pozit√≠v eg√©sznek kell lennie.")
        return

    process_directory_final_v4(
        directory_path=args.input_directory,
        auto_chunk=not args.no_auto_chunk,
        fixed_chunk_s=args.chunk,
        fixed_batch_size=args.batch,
    )

if __name__ == "__main__":
    main()
